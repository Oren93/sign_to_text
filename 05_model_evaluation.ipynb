{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models training\n",
    "In this notebook we train and analyze LSTM models, compare them and tune them to get the best results. We will work with a small subset due to low hardware availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load variable\n",
    "This notebook is a direct continuation of data_preprocessing.ipynb. We start by loading the necessary libraries variables from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastdtw import fastdtw\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from landmarks_augmentator import produce_augmentations\n",
    "\n",
    "with open('data/landmarks_subset.pkl', 'rb') as file:\n",
    "    subset_landmarks = pickle.load(file)\n",
    "for key in list(subset_landmarks.keys()):\n",
    "    if len(subset_landmarks[key]) == 0:\n",
    "        del subset_landmarks[key]\n",
    "\n",
    "data_info = pd.read_csv('data/video_labels.csv',dtype={'video_id': object},index_col=0)\n",
    "data_info = data_info.loc[data_info.video_id.isin(subset_landmarks.keys())]\n",
    "subset_words = data_info.loc[data_info.video_id.isin(subset_landmarks.keys()),'word'].unique()\n",
    "\n",
    "POSE = np.hstack((np.ones(33), np.zeros(21+21+468))) == 1\n",
    "LH = np.hstack((np.zeros(33), np.ones(21), np.zeros(21+468))) == 1\n",
    "RH = np.hstack((np.zeros(33+21), np.ones(21), np.zeros(468))) == 1\n",
    "FACE = np.hstack((np.zeros(33+21+21), np.ones(468))) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Test-train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of hyperparameters:\n",
    "- **Dimensions** Check wether 3D is significanly more sufficient than 2D\n",
    "- **Face landmarks** Do the face landmarks contribute much? If yes, how many of them should we take?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKED_FRAMES = 24 # min([len(video) for video in landmarks.values()])\n",
    "DIMENSTIONS = 3\n",
    "USE_AUGMENTATIONS = True# Use augmented videos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_signer_ids = [118, 31, 59, 11, 115, 94, 6, 21, 10, 38, 56, 41, 4, 45, 32, 46, 13,\n",
    "#                    42, 39, 17, 89, 60, 35, 15, 3, 92, 93, 34, 107, 28, 99, 37, 8, 97,\n",
    "#                    70, 19, 91, 106, 63, 29, 26, 117, 66, 119, 50, 103, 120, 95, 78, 27,\n",
    "#                    108, 57, 53, 75, 104, 43, 40, 77, 1, 33, 22, 105, 48, 73, 23]\n",
    "#val_signer_ids = [2, 52, 12, 98, 88]\n",
    "#test_signer_ids = [59, 115, 90, 4, 116, 100, 101, 102, 96, 90]\n",
    "#\n",
    "#num_of_words = data_info[data_info[\"word\"].isin(subset_words)][\"word\"].nunique()\n",
    "#subset_data = data_info[data_info[\"word\"].isin(subset_words)]\n",
    "#subset_train_data = subset_data[subset_data[\"signer_id\"].isin(train_signer_ids)]\n",
    "#subset_test_data = subset_data[subset_data[\"signer_id\"].isin(val_signer_ids)]\n",
    "#subset_validation_data = subset_data[subset_data[\"signer_id\"].isin(test_signer_ids)]\n",
    "#\n",
    "#print(f\"subset train data count: {len(subset_train_data)}\")\n",
    "#print(f\"subset test data count: {len(subset_test_data)}\")\n",
    "#print(f\"subset validation data count: {len(subset_validation_data)}\")\n",
    "#print(f\"num of words: {num_of_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, and test as given by the data authors, should find a better split\n",
    "train_ids = list(data_info.loc[data_info.split=='train','video_id'])\n",
    "validation_ids = list(data_info.loc[data_info.split=='val','video_id'])\n",
    "test_ids = list(data_info.loc[data_info.split=='test','video_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Produce Augmentations \n",
    "For train test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_landmarks = produce_augmentations(subset_landmarks,data_info)\n",
    "train_landmarks = {id:video for id, video in subset_landmarks.items() if id in train_ids}\n",
    "train_landmarks = produce_augmentations(train_landmarks,data_info.loc[data_info.video_id.isin(train_landmarks.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Prepare data to fit in LSTM\n",
    "Take same amount of frames from each video.\n",
    "<p style=\"color:red;\">hopefully this is a temporary phase in the development </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_frames(video,num_frames):\n",
    "    ''' Take a subset of the frames, evenly spread over the whole video\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "        EDIT: After getting the missing data, some video are too short and need more frames instad\n",
    "              Will it bias the predictions? Nee to consider discarding these examples.\n",
    "              OR ask chatGPT to write a function to make it a smooth \"slow motion\", I don't have time for this\n",
    "    ----------------------------------------------------------------------------------------------------------\n",
    "    '''\n",
    "    if len(video) < num_frames:\n",
    "        video_longer = video.copy()\n",
    "        for _ in range(len(video),num_frames):\n",
    "            video_longer = np.append(video_longer,video_longer[-1])\n",
    "        return video_longer\n",
    "    step_size = len(video) // num_frames\n",
    "    video_shorter = video[::step_size][:num_frames]\n",
    "    return video_shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/bbs_fbvd6bz2p7tm527x836m0000gn/T/ipykernel_10192/1135187795.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  augmented_row.video_id = id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16971 videos in in total for 494 words\n"
     ]
    }
   ],
   "source": [
    "words = subset_words \n",
    "\n",
    "#lstm_landmarks = {}\n",
    "#for word in words:\n",
    "#    videos = data_info.loc[data_info.word==word,'video_id']\n",
    "#    for vid in videos:\n",
    "#        lstm_landmarks[vid] = pick_frames(subset_landmarks[vid][:,POSE+LH+RH,:DIMENSTIONS].copy(),PICKED_FRAMES)\n",
    "#        for key in subset_landmarks:\n",
    "#            if key.split('_')[0] == vid:\n",
    "#                lstm_landmarks[vid] = pick_frames(subset_landmarks[vid][:,POSE+LH+RH,:DIMENSTIONS].copy(),PICKED_FRAMES)\n",
    "for id, video in list(subset_landmarks.items()):\n",
    "    if id not in train_ids:\n",
    "        subset_landmarks[id] = pick_frames(video[:,POSE+LH+RH,:DIMENSTIONS],PICKED_FRAMES)\n",
    "    else: # Clear up space, maybe irrelevant because we soon delete the whole subset_landmarks object\n",
    "        del subset_landmarks[id]\n",
    "\n",
    "for id, video in train_landmarks.items():\n",
    "        train_landmarks[id] = pick_frames(video[:,POSE+LH+RH,:DIMENSTIONS],PICKED_FRAMES)\n",
    "        original_id = id.split('_')[0]\n",
    "        if original_id != id and id not in data_info.video_id: # if augmented type, add its label to the data_info table\n",
    "            augmented_row = data_info.loc[data_info.video_id==original_id,:]\n",
    "            augmented_row.video_id = id\n",
    "            data_info = pd.concat([data_info, augmented_row])\n",
    "\n",
    "print(f'{len(subset_landmarks)+len(train_landmarks)} videos in in total for {len(words)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_tr = {}\n",
    "for video_id, video in train_landmarks.items():\n",
    "    foo_tr[video_id] = np.array([frame.flatten() for frame in video])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/bbs_fbvd6bz2p7tm527x836m0000gn/T/ipykernel_10192/252432371.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.array([video for video in train_landmarks.values()])\n",
      "/var/folders/sl/bbs_fbvd6bz2p7tm527x836m0000gn/T/ipykernel_10192/252432371.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_val = np.array([video for id, video in subset_landmarks.items() if id in validation_ids])\n",
      "/var/folders/sl/bbs_fbvd6bz2p7tm527x836m0000gn/T/ipykernel_10192/252432371.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test = np.array([video for id, video in subset_landmarks.items() if id in test_ids])\n"
     ]
    }
   ],
   "source": [
    "for video_id, video in train_landmarks.items():\n",
    "    train_landmarks[video_id] = np.array([frame.flatten() for frame in video])\n",
    "for video_id, video in subset_landmarks.items():\n",
    "    subset_landmarks[video_id] = np.array([frame.flatten() for frame in video])\n",
    "\n",
    "X_train = np.array([video for video in train_landmarks.values()])\n",
    "y_train = [data_info.loc[data_info.video_id==id,'word'].item() for id in train_landmarks.keys()]\n",
    "\n",
    "X_val = np.array([video for id, video in subset_landmarks.items() if id in validation_ids])\n",
    "y_val = [data_info.loc[data_info.video_id==id,'word'].item() for id in subset_landmarks.keys() if id in validation_ids]\n",
    "\n",
    "X_test = np.array([video for id, video in subset_landmarks.items() if id in test_ids])\n",
    "y_test = [data_info.loc[data_info.video_id==id,'word'].item() for id in subset_landmarks.keys() if id in test_ids]\n",
    "\n",
    "del subset_landmarks, train_landmarks\n",
    "# Suffle the train set\n",
    "num_instances = X_train.shape[0]\n",
    "shuffled_indices = np.arange(num_instances)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "X_train = X_train[shuffled_indices]\n",
    "y_train = np.array(y_train)[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mstr\u001b[39m(MODEL_VERSION))\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_tf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m index_to_word \u001b[38;5;241m=\u001b[39m {word: i \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words)}\n\u001b[1;32m     18\u001b[0m y_train_categorical \u001b[38;5;241m=\u001b[39m to_categorical([index_to_word[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m y_train], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(words))\n",
      "Cell \u001b[0;32mIn[54], line 12\u001b[0m, in \u001b[0;36mload_tf_model\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241m.\u001b[39mload_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path,file))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "def load_tf_model(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        return False\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.keras'):\n",
    "            return keras.saving.load_model(os.path.join(folder_path,file))\n",
    "    return False\n",
    "folder_path = os.path.join(\"serving\",\"lstm\",str(MODEL_VERSION))\n",
    "model = load_tf_model(folder_path)\n",
    "\n",
    "index_to_word = {word: i for i, word in enumerate(words)}\n",
    "y_train_categorical = to_categorical([index_to_word[word] for word in y_train], num_classes=len(words))\n",
    "if not model:    \n",
    "    input_shape = (X_train[0].shape)  #Hopefully we can do variable number of frames later\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train_categorical, epochs=100, batch_size=8)\n",
    "    model.save(os.path.join('serving','lstm',str(MODEL_VERSION),'sign_to_text.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find how many correctly classified instances we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>correct_count</th>\n",
       "      <th>appeared_test</th>\n",
       "      <th>train_instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tall</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cold</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pizza</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dark</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wear</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word correct_count appeared_test train_instances\n",
       "0   tall             4             8              40\n",
       "1    man             4             8              40\n",
       "2   cold             8            12              28\n",
       "3  pizza             2             8              32\n",
       "4   dark             4             4              36\n",
       "5   wear             5            10              15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean and median amount of instances per word in the train set are: 27.225806451612904 28.0\n"
     ]
    }
   ],
   "source": [
    "count_train = Counter(y_train)\n",
    "def per_stats(X_test = X_test,y_test = y_test):\n",
    "    correct = {}\n",
    "    for i,j in enumerate(model.predict(X_test).argmax(axis=1)):\n",
    "        if y_test[i] == words[j]:\n",
    "            correct[y_test[i]] = correct.get(y_test[i],0) + 1\n",
    "    count_test = Counter(y_test)\n",
    "    pred_stats = []\n",
    "    for word, count in correct.items():\n",
    "        row_data = {\"word\": word, \"correct_count\": count, \"appeared_test\": count_test.get(word, 0), \"train_instances\": count_train.get(word, 0)}\n",
    "        pred_stats.append(row_data)\n",
    "\n",
    "    # Concatenate the list of dictionaries to the DataFrame\n",
    "    pred_stats = pd.concat([ pd.DataFrame(columns=[\"word\",\"correct_count\",\"appeared_test\",\"train_instances\"]), pd.DataFrame(pred_stats)], ignore_index=True)\n",
    "    return pred_stats\n",
    "display(per_stats(X_test = X_val,y_test = y_val))\n",
    "print(\"The mean and median amount of instances per word in the train set are:\",np.mean(list(count_train.values())),np.median(list(count_train.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that to get better reults we need more augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for self use, get video ids for a word in a particular set\n",
    "def find_video_id(word,set = validation_ids):\n",
    "    df = pd.concat([data_info.loc[data_info.video_id==id,['video_id','word']] for id in lstm_landmarks.keys() if id in set])\n",
    "    return df.loc[df.word == word]\n",
    "#use like this:\n",
    "#find_video_id('short',validation_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 LSTM Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "106/106 [==============================] - 12s 68ms/step - loss: 3.5270 - categorical_accuracy: 0.0462\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 7s 64ms/step - loss: 3.3592 - categorical_accuracy: 0.0581\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 3.0617 - categorical_accuracy: 0.0652\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 2.9307 - categorical_accuracy: 0.0841\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 2.8648 - categorical_accuracy: 0.0900\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 22.1359 - categorical_accuracy: 0.0995\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 2.8907 - categorical_accuracy: 0.1066\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 2.8154 - categorical_accuracy: 0.1055\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 8s 71ms/step - loss: 2.6906 - categorical_accuracy: 0.1398\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 2.6108 - categorical_accuracy: 0.1600\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 2.5203 - categorical_accuracy: 0.1813\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 7s 71ms/step - loss: 2.3931 - categorical_accuracy: 0.2204\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 2.2881 - categorical_accuracy: 0.2393\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 2.3176 - categorical_accuracy: 0.2500\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 2.1568 - categorical_accuracy: 0.2855\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 2.0448 - categorical_accuracy: 0.3235\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 2.0157 - categorical_accuracy: 0.3400\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 12s 116ms/step - loss: 1.8701 - categorical_accuracy: 0.3673\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 45.7852 - categorical_accuracy: 0.3460\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 1.9529 - categorical_accuracy: 0.3187\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 1.7017 - categorical_accuracy: 0.4182\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 9s 84ms/step - loss: 1.8903 - categorical_accuracy: 0.3780\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 9s 80ms/step - loss: 1.6570 - categorical_accuracy: 0.4242\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 1.4268 - categorical_accuracy: 0.4751\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 1.3228 - categorical_accuracy: 0.5095\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 1.2595 - categorical_accuracy: 0.5427\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 1.3887 - categorical_accuracy: 0.4941\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 1.1562 - categorical_accuracy: 0.5841\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 1.2023 - categorical_accuracy: 0.5841\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.8947 - categorical_accuracy: 0.4455\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 7s 67ms/step - loss: 1.1329 - categorical_accuracy: 0.5936\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 1.1508 - categorical_accuracy: 0.5853\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 1.1145 - categorical_accuracy: 0.5900\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 0.8138 - categorical_accuracy: 0.7014\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 1.0162 - categorical_accuracy: 0.6493\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 0.7842 - categorical_accuracy: 0.7156\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 0.7306 - categorical_accuracy: 0.7524\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 7s 69ms/step - loss: 0.6720 - categorical_accuracy: 0.7642\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 0.6507 - categorical_accuracy: 0.7666\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.7876 - categorical_accuracy: 0.7405\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.4734 - categorical_accuracy: 0.8472\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.6040 - categorical_accuracy: 0.7938\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 8s 73ms/step - loss: 0.6315 - categorical_accuracy: 0.7927\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.4466 - categorical_accuracy: 0.8602\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 8s 80ms/step - loss: 0.4515 - categorical_accuracy: 0.8294\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.5191 - categorical_accuracy: 0.8235\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 0.4340 - categorical_accuracy: 0.8472\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 7s 71ms/step - loss: 0.3451 - categorical_accuracy: 0.8815\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.4403 - categorical_accuracy: 0.8685\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.2465 - categorical_accuracy: 0.9230\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 34.8084 - categorical_accuracy: 0.4775\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 11s 100ms/step - loss: 2.3260 - categorical_accuracy: 0.2512\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 1.4873 - categorical_accuracy: 0.4858\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 1.0901 - categorical_accuracy: 0.6137\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 7s 70ms/step - loss: 0.8611 - categorical_accuracy: 0.7062\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.5944 - categorical_accuracy: 0.7844\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 9s 90ms/step - loss: 0.6617 - categorical_accuracy: 0.7571\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.4447 - categorical_accuracy: 0.8602\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.3875 - categorical_accuracy: 0.8720\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 8s 72ms/step - loss: 0.4346 - categorical_accuracy: 0.8780\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.2996 - categorical_accuracy: 0.8981\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.3897 - categorical_accuracy: 0.8720\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.4700 - categorical_accuracy: 0.8389\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 8s 71ms/step - loss: 0.2578 - categorical_accuracy: 0.9064\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 7s 68ms/step - loss: 0.1210 - categorical_accuracy: 0.9656\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.0965 - categorical_accuracy: 0.9656\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.5151 - categorical_accuracy: 0.8400\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 0.2405 - categorical_accuracy: 0.9171\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.1700 - categorical_accuracy: 0.9479\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.0935 - categorical_accuracy: 0.9692\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.0842 - categorical_accuracy: 0.9846\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 8s 74ms/step - loss: 0.1620 - categorical_accuracy: 0.9502\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.1195 - categorical_accuracy: 0.9680\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.2981 - categorical_accuracy: 0.9100\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.3375 - categorical_accuracy: 0.9111\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.0596 - categorical_accuracy: 0.9882\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 8s 71ms/step - loss: 1.0789 - categorical_accuracy: 0.8886\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 1.2026 - categorical_accuracy: 0.5853\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 10s 95ms/step - loss: 0.5303 - categorical_accuracy: 0.8258\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 9s 87ms/step - loss: 0.4719 - categorical_accuracy: 0.8199\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.3822 - categorical_accuracy: 0.8637\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 0.3413 - categorical_accuracy: 0.8922\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.3030 - categorical_accuracy: 0.8969\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 10s 90ms/step - loss: 0.1569 - categorical_accuracy: 0.9573\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.1528 - categorical_accuracy: 0.9562\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.3255 - categorical_accuracy: 0.9005\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.2906 - categorical_accuracy: 0.8993\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 10s 96ms/step - loss: 0.0881 - categorical_accuracy: 0.9763\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 9s 81ms/step - loss: 0.0219 - categorical_accuracy: 0.9988\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.0702 - categorical_accuracy: 0.9799\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.2434 - categorical_accuracy: 0.9419\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 9s 86ms/step - loss: 0.2098 - categorical_accuracy: 0.9360\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 11s 100ms/step - loss: 0.0264 - categorical_accuracy: 0.9988\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 9s 82ms/step - loss: 0.0097 - categorical_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 9s 88ms/step - loss: 0.0048 - categorical_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 11s 99ms/step - loss: 0.0029 - categorical_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 9s 89ms/step - loss: 0.0021 - categorical_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 10s 94ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 8s 79ms/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 8s 77ms/step - loss: 9.6067e-04 - categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "MODEL_VERSION = 2\n",
    "folder_path = os.path.join(\"serving\",\"lstm\",str(MODEL_VERSION))\n",
    "model = load_tf_model(folder_path)\n",
    "\n",
    "if not model:    \n",
    "    input_shape = (lstm_landmarks[list(lstm_landmarks.keys())[0]].shape)  #Hopefully we can do variable number of frames later\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "    model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(len(words), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train_categorical, epochs=100, batch_size=8)\n",
    "    model.save(os.path.join('serving','lstm',str(MODEL_VERSION),'sign_to_text.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>correct_count</th>\n",
       "      <th>appeared_test</th>\n",
       "      <th>train_instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tall</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>taste</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dark</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>room</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dress</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word correct_count appeared_test train_instances\n",
       "0   tall             4             8              40\n",
       "1    man             4             8              40\n",
       "2  taste             5             5              25\n",
       "3   dark             4             4              36\n",
       "4   room             4             8              28\n",
       "5  dress             5            10              20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean and median amount of instances per word in the train set are: 27.225806451612904 28.0\n"
     ]
    }
   ],
   "source": [
    "display(per_stats(X_test = X_val,y_test = y_val))\n",
    "print(\"The mean and median amount of instances per word in the train set are:\",np.mean(list(count_train.values())),np.median(list(count_train.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
